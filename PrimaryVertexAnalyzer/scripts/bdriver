#!/usr/bin/env python
"""
script to create area for submission of HTCondor batch jobs
"""
import argparse
import os
import stat
import sys
import math
import json
import datetime

from usercode.PrimaryVertexAnalyzer.utils.common import *
from usercode.PrimaryVertexAnalyzer.utils.das import load_dataset_data, skim_das_jsondump

#### main
if __name__ == '__main__':
    ### args
    parser = argparse.ArgumentParser(
     prog='./'+os.path.basename(__file__),
     formatter_class=argparse.RawDescriptionHelpFormatter,
     description=__doc__)

    parser.add_argument('-c', '--cfg', dest='cmsRun_cfg', action='store', default=None, required=True,
                        help='path to cmsRun cfg file')

    parser.add_argument('--customize-cfg', dest='customize_cfg', action='store_true', default=False,
                        help='append minimal customization to cmsRun-cfg required by the driver')

    parser.add_argument('-d', '--dataset', dest='dataset', action='store', default=None, required=True,
                        help='name of input data set in DAS')

    parser.add_argument('-o', '--output', dest='output', action='store', default=None, required=True,
                        help='path to output directory')

    parser.add_argument('--cmsRun-output-dir', dest='cmsRun_output_dir', action='store', default=None,
                        help='directory to host output of cmsRun job (if not specified, same as --output)')

    parser.add_argument('-j', '--jobdirname', dest='jobdirname', action='store', default='job',
                        help='prefix of batch-job sub-directories (example: [JOBDIRNAME]_[counter]/)')

    parser.add_argument('-f', '--max-files', dest='max_files', action='store', type=int, default=-1,
                        help='maximum number of input files to be processed (if integer is negative, all files will be processed)')

    parser.add_argument('-m', '--max-events', dest='max_events', action='store', type=int, default=-1,
                        help='maximum number of total input events to be processed (if integer is negative, all events will be processed)')

    parser.add_argument('-n', '--n-events', dest='n_events', action='store', type=int, default=-1,
                        help='maximum number of events per job')

    parser.add_argument('-p', '--parentFiles-level', dest='parentFiles_level', action='store', type=int, default=2,
                        help='level of parentFiles to be used as secondary input files (currently, anything other than 1 and 2 disables the usage of secondary input files)')

    parser.add_argument('--cpus', dest='cpus', action='store', type=int, default=1,
                        help='argument of HTCondor parameter "RequestCpus"')

    parser.add_argument('--memory', dest='memory', action='store', type=int, default=2000,
                        help='argument of HTCondor parameter "RequestMemory"')

    parser.add_argument('--runtime', dest='runtime', action='store', type=int, default=10800,
                        help='argument of HTCondor parameter "+RequestRuntime"')

    parser.add_argument('--JobFlavour', dest='JobFlavour', action='store', default=None,
                        help='argument of HTCondor parameter "+JobFlavour" (by default, the parameter is not specified)')

    parser.add_argument('--AccountingGroup', dest='AccountingGroup', action='store', default=None,
                        help='argument of HTCondor parameter "+AccountingGroup" (by default, the parameter is not specified)')

    parser.add_argument('--batch-name', dest='batch_name', action='store', default=None,
                        help='argument of HTCondor parameter "batch_name" (if not specified, basename of argument of "--output" is used)')

    parser.add_argument('--submit', dest='submit', action='store_true', default=False,
                        help='submit job(s) on the batch system')

    parser.add_argument('--export-LD-LIBRARY-PATH', dest='export_LD_LIBRARY_PATH', action='store_true', default=False,
                        help='export explicitly the environment variable "LD_LIBRARY_PATH" in the batch-job executable')

    parser.add_argument('--dry-run', dest='dry_run', action='store_true', default=False,
                        help='enable dry-run mode')

    parser.add_argument('-v', '--verbose', dest='verbose', action='store_true', default=False,
                        help='enable verbose mode')

    opts, opts_unknown = parser.parse_known_args()
    ### -------------------------

    log_prx = os.path.basename(__file__)+' -- '

    ### opts --------------------
    if 'CMSSW_BASE' not in os.environ:
       KILL(log_prx+'global variable CMSSW_BASE is not defined (set up CMSSW environment with "cmsenv" before submitting jobs)')

    if not os.path.isfile(opts.cmsRun_cfg):
       KILL(log_prx+'invalid path to cmsRun cfg file [-c]: '+str(opts.cmsRun_cfg))

    if os.path.exists(opts.output):
       KILL(log_prx+'target path to output directory already exists [-o]: '+str(opts.output))

    OUTPUT_DIR = os.path.abspath(opts.output)

    if opts.n_events == 0:
       KILL(log_prx+'logic error: requesting zero events per job (use non-zero value for argument of option "-n")')

    if opts.submit and (not opts.dry_run):
       which('condor_submit')

    if opts.cpus <= 0:
       KILL(log_prx+'invalid (non-positive) value for HTCondor parameter "RequestCpus": '+str(opts.cpus))

    if opts.memory <= 0:
       KILL(log_prx+'invalid (non-positive) value for HTCondor parameter "RequestMemory": '+str(opts.memory))

    if opts.runtime <= 0:
       KILL(log_prx+'invalid (non-positive) value for HTCondor parameter "+RequestRuntime": '+str(opts.runtime))

    is_slc7_arch = False
    if os.environ['SCRAM_ARCH'].startswith('slc7'): is_slc7_arch = True
    elif os.environ['SCRAM_ARCH'].startswith('slc6'): pass
    else:
       KILL(log_prc+'could not infer architecture from environment variable "SCRAM_ARCH": '+str(os.environ['SCRAM_ARCH']))

    if opts.max_events == 0:
       KILL(log_prx+'logic error: requesting a maximum of zero input events (use non-zero value for argument of option --max-events/-m)')

    if opts.max_files == 0:
       KILL(log_prx+'logic error: requesting a maximum of zero input files (use non-zero value for argument of option --max-files/-f)')

    ### unrecognized command-line arguments
    ### -> used as additional command-line arguments to cmsRun
    cmsRun_addopts = opts_unknown[:]

    if opts.verbose and len(cmsRun_addopts):
       print '-'*50
       print colored_text('additional cmsRun command-line arguments:', ['1'])
       for _tmp in cmsRun_addopts: print ' '+str(_tmp)
       print '-'*50

    ### extract input-files information from data set name via dasgoclient
    ### -> list of dictionaries, each containing DAS name, files, number of events per file, and parent files
    input_dset = {}

    if os.path.isfile(opts.dataset):
       input_dset = skim_das_jsondump(file_path=opts.dataset, max_files=opts.max_files, max_events=opts.max_events, verbose=opts.verbose)
    else:
       which('dasgoclient')
       input_dset = load_dataset_data(das_name=opts.dataset, max_files=opts.max_files, max_events=opts.max_events, parentFiles_levels=opts.parentFiles_level, verbose=opts.verbose)

    ### total number of events and jobs
    ### -> used to determine format of output-file name
    nJobs, totEvents, breakLoop = 0, 0, False
    for i_inpfdc in input_dset['files']:
        i_nevents = i_inpfdc['nevents']
        totEvents += i_nevents
        if (opts.max_events > 0) and (totEvents >= opts.max_events):
           i_nevents -= (totEvents - opts.max_events)
           breakLoop = True
        nJobs += int(math.ceil(float(i_nevents) / opts.n_events)) if (opts.n_events > 0) else 1
        if breakLoop:
           break
    if nJobs == 0:
       KILL(log_prx+'input error: expected number of batch jobs is zero (check input data set and number of events): '+opts.dataset)
    outputname_postfix_format = '_{:0'+str(max(1, int(math.ceil(math.log10(nJobs)))))+'d}'
    del nJobs, totEvents, breakLoop

    ### voms proxy
    voms_proxy_path = None

    if ('X509_USER_PROXY' in os.environ):
       voms_proxy_path = os.environ['X509_USER_PROXY']
    else:
       voms_proxy_path = '/tmp/x509up_u'+str(os.getuid())

    if not os.path.isfile(voms_proxy_path):
       EXE('voms-proxy-init --voms cms', verbose=opts.verbose, dry_run=opts.dry_run)

    if not os.path.isfile(voms_proxy_path):
       KILL(log_prx+'invalid path to voms proxy: '+voms_proxy_path)

    MKDIRP(OUTPUT_DIR, verbose=opts.verbose, dry_run=opts.dry_run)
    EXE('cp '+voms_proxy_path+' '+OUTPUT_DIR+'/X509_USER_PROXY', verbose=opts.verbose, dry_run=opts.dry_run)

    if opts.cmsRun_output_dir is not None:
      MKDIRP(opts.cmsRun_output_dir, verbose=opts.verbose, dry_run=opts.dry_run)

    ### copy cmsRun-cfg into output directory
    out_cmsRun_cfg = os.path.abspath(OUTPUT_DIR+'/cfg.py')
    EXE('cp '+opts.cmsRun_cfg+' '+out_cmsRun_cfg, verbose=opts.verbose, dry_run=opts.dry_run)

    if opts.customize_cfg:
       with open(out_cmsRun_cfg, 'a') as cfg_file:
            custom_str = """
###
### customization added by {:} [time-stamp: {:}]
###
import FWCore.ParameterSet.VarParsing as vpo
opts = vpo.VarParsing('analysis')

opts.register('skipEvents', 0,
              vpo.VarParsing.multiplicity.singleton,
              vpo.VarParsing.varType.int,
              'number of events to be skipped')

opts.register('output', 'out.root',
              vpo.VarParsing.multiplicity.singleton,
              vpo.VarParsing.varType.string,
              'path to output file of TFileService')

opts.parseArguments()

# max number of events to be processed
process.maxEvents.input = opts.maxEvents

# number of events to be skipped
process.source.skipEvents = cms.untracked.uint32(opts.skipEvents)

# input EDM files [primary]
process.source.fileNames = opts.inputFiles

# input EDM files [secondary]
process.source.secondaryFileNames = opts.secondaryInputFiles

# output [TFileService]
if hasattr(process, 'TFileService'):
  process.TFileService.fileName = opts.output
"""
            cfg_file.write(custom_str.format(os.path.basename(__file__), str(datetime.datetime.now())))

    ### copy driver command
    if not opts.dry_run:
      with open(os.path.abspath(OUTPUT_DIR+'/cmdLog'), 'w') as file_cmdLog:
        file_cmdLog.write((' '.join(sys.argv[:]))+'\n')

    ### copy dataset information in .json format
    if not opts.dry_run:
       json.dump(input_dset, open(OUTPUT_DIR+'/dataset.json', 'w'), sort_keys=True, indent=2)

    ### dump of python config
    retcode = EXE('edmConfigDump '+out_cmsRun_cfg+' 2> '+OUTPUT_DIR+'/configDump.log > '+OUTPUT_DIR+'/configDump.py', suspend=False, verbose=opts.verbose, dry_run=opts.dry_run)
    if retcode:
       EXE('cat '+OUTPUT_DIR+'/configDump.log', verbose=opts.verbose, dry_run=opts.dry_run)
       raise SystemExit(retcode)
    del retcode

    ### condor submission file
    SUBFILE_ABSPATH = OUTPUT_DIR+'/condor.sub'

    if os.path.exists(SUBFILE_ABSPATH):
       KILL(log_prx+'target output file already exists (condor submission file): '+SUBFILE_ABSPATH)

    SUBFILE_LINES = [
      'BASEDIR = '+OUTPUT_DIR,
      'batch_name = '+(opts.batch_name if (opts.batch_name is not None) else '$Fn(BASEDIR)'),
      'initialdir = $DIRNAME(QFILE)',
      'executable = $(initialdir)/exe.sh',
      'output = logs/out.$(Cluster).$(Process)',
      'error  = logs/err.$(Cluster).$(Process)',
      'log    = logs/log.$(Cluster).$(Process)',
      '#arguments =',
      '#transfer_executable = True',
      '#transfer_input_files =',
      'universe = vanilla',
      'getenv = True',
      'should_transfer_files = IF_NEEDED',
      'when_to_transfer_output = ON_EXIT',
      'requirements = (OpSysAndVer == "'+('CentOS7' if is_slc7_arch else 'SL6')+'")',
      'RequestCpus = '+str(opts.cpus),
      'RequestMemory = '+str(opts.memory),
      '+RequestRuntime = '+str(opts.runtime),
    ]

    if opts.JobFlavour is not None:
       JobFlavour = opts.JobFlavour
       while JobFlavour.startswith("'") or JobFlavour.startswith('"'):
         JobFlavour = JobFlavour[1:]
       while JobFlavour.endswith("'") or JobFlavour.endswith('"'):
         JobFlavour = JobFlavour[:-1]
       SUBFILE_LINES += [
         '+JobFlavour = "{:}"'.format(JobFlavour)
       ]

    if opts.AccountingGroup is not None:
       AccountingGroup = opts.AccountingGroup
       while AccountingGroup.startswith("'") or AccountingGroup.startswith('"'):
         AccountingGroup = AccountingGroup[1:]
       while AccountingGroup.endswith("'") or AccountingGroup.endswith('"'):
         AccountingGroup = AccountingGroup[:-1]
       SUBFILE_LINES += [
         '+AccountingGroup = "{:}"'.format(AccountingGroup)
       ]

    SUBFILE_LINES += [
      'x509userproxy = $(BASEDIR)/X509_USER_PROXY',
      'queue QFILE matching files $(BASEDIR)/*/flag.queue',
    ]

    if not opts.dry_run:
       with open(SUBFILE_ABSPATH, 'w') as f_subfile:
          for _tmp in SUBFILE_LINES:
              f_subfile.write(_tmp+'\n')

    ### create output batch scripts
    jobCounter = 0
    totEvents = 0
    breakLoop = False

    for i_inpfdc in input_dset['files']:

        i_inputFile = i_inpfdc['file']
        i_inputFile_nevents = i_inpfdc['nevents']

        i_secondaryInputFiles = None
        if opts.parentFiles_level == 2:
           i_secondaryInputFiles = i_inpfdc['parentFiles_2']
        elif opts.parentFiles_level == 1:
           i_secondaryInputFiles = i_inpfdc['parentFiles_1']

        if isinstance(i_secondaryInputFiles, list) and len(i_secondaryInputFiles) == 0:
           KILL(log_prx+'list of secondary input files is empty (parentFiles_level = '+str(opts.parentFiles_level)+')')

        # number of jobs for this set of input files
        i_njobs = int(math.ceil(float(i_inputFile_nevents) / opts.n_events)) if (opts.n_events > 0) else 1
        i_nevt_remainder = i_inputFile_nevents%opts.n_events if (opts.n_events > 0) else 0

        for i_job in range(i_njobs):

            # name of output sub-directory
            i_OUTPUT_DIR = OUTPUT_DIR+'/'+opts.jobdirname+outputname_postfix_format.format(jobCounter)

            # create logs/ directory for HTCondor log-files
            MKDIRP(i_OUTPUT_DIR+'/logs', verbose=opts.verbose, dry_run=opts.dry_run)

            # create (empty) queue-file to trigger job submission
            EXE('touch '+i_OUTPUT_DIR+'/flag.queue', verbose=opts.verbose, dry_run=opts.dry_run)

            # number of events for this job
            i_maxEvents = opts.n_events if (opts.n_events > 0) else i_inputFile_nevents
            if (i_job == (i_njobs - 1)) and (i_nevt_remainder != 0):
               i_maxEvents = i_nevt_remainder

            totEvents += i_maxEvents
            if (opts.max_events > 0) and (totEvents >= opts.max_events):
               i_maxEvents -= (totEvents - opts.max_events)
               breakLoop = True

            EXEFILE_ABSPATH = i_OUTPUT_DIR+'/exe.sh'

            if os.path.exists(EXEFILE_ABSPATH):
               KILL(log_prx+'target output file already exists (executable of batch job): '+EXEFILE_ABSPATH)

            if not opts.dry_run:
               with open(EXEFILE_ABSPATH, 'w') as f_exefile:
                  f_exefile.write('#!/bin/bash\n')

                  # export explicitly the environment variable LD_LIBRARY_PATH
                  if opts.export_LD_LIBRARY_PATH:
                     if 'LD_LIBRARY_PATH' in os.environ:
                        f_exefile.write('\n'+'export LD_LIBRARY_PATH='+os.environ['LD_LIBRARY_PATH']+'\n')

                  # cmsRun arguments (cfg-file + options)
                  cmsRun_opts = os.path.relpath(out_cmsRun_cfg, i_OUTPUT_DIR)
                  cmsRun_opts += ' \\\n maxEvents='+str(i_maxEvents)
                  cmsRun_opts += ' \\\n skipEvents='+str(opts.n_events*i_job)
                  cmsRun_opts += ' \\\n inputFiles='+str(i_inputFile)
                  if i_secondaryInputFiles:
                    cmsRun_opts += ' \\\n secondaryInputFiles='+str(','.join(i_secondaryInputFiles))

                  CMSRUN_OUTPUT_DIR = opts.cmsRun_output_dir
                  if CMSRUN_OUTPUT_DIR is None:
                    CMSRUN_OUTPUT_DIR = '.'
                  elif os.path.isabs(CMSRUN_OUTPUT_DIR):
                    while CMSRUN_OUTPUT_DIR.endswith('/') and len(CMSRUN_OUTPUT_DIR) > 1:
                      CMSRUN_OUTPUT_DIR = CMSRUN_OUTPUT_DIR[:-1]
                  else:
                    CMSRUN_OUTPUT_DIR = os.path.relpath(os.path.abspath(CMSRUN_OUTPUT_DIR), i_OUTPUT_DIR)
                  cmsRun_opts += ' \\\n output='+CMSRUN_OUTPUT_DIR+'/out'+outputname_postfix_format.format(jobCounter)+'.root'

                  for _tmp in cmsRun_addopts:
                    cmsRun_opts += ' \\\n '+str(_tmp)

                  i_SHELL_COMMANDS = [
                   ['set -e'],
                   ['cd '+os.environ['CMSSW_BASE']+'/src'],
                   ['eval `scram runtime -sh`'],
                   ['cd '+i_OUTPUT_DIR],
                   ['[ ! -f flag.done ] || exit 0'],
                   ['cmsRun '+cmsRun_opts],
                   ['touch flag.done'],
                  ]

                  f_exefile.write('\n'+('\n\n'.join([' \\\n '.join(_tmp) for _tmp in i_SHELL_COMMANDS]))+'\n')

            st = os.stat(EXEFILE_ABSPATH)
            os.chmod(EXEFILE_ABSPATH, st.st_mode | stat.S_IEXEC)

            print colored_text('output:', ['1', '94']), os.path.relpath(EXEFILE_ABSPATH, os.environ['PWD'])

            jobCounter += 1

            if breakLoop:
               break

    del jobCounter, totEvents, breakLoop

    if opts.submit:
       EXE('condor_submit '+SUBFILE_ABSPATH, suspend=False, verbose=opts.verbose, dry_run=opts.dry_run)
